{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#0 : Promotional  5.33% æŠ˜æ‰£ã€ä¸‹æ®º ç‰¹åƒ¹ã€‚ã€‚ã€‚\n",
    "#1: Engagement  31.56%   æŒ‰è®šã€æ‰“å¡ åˆ†äº«ã€‚ã€‚ã€‚\n",
    "#2: Product Awareness  28% (ç”¢å“) â€œnew productâ€ ç³»åˆ—ã€é¦–ç™¼ã€æ–°ç™»å ´ã€é–‹ç®±ã€æ¨è–¦ æ–°ä¸€ä»£ é€²åŒ–ç‰ˆ\n",
    "#3: Brand Awareness 8.44%ï¼ˆå“ç‰Œï¼‰\n",
    "#Corporate Social Responsibility 5.33%\n",
    "#Customer Service 3.11% (ç²‰çµ²) æœå‹™ å“ç®¡\n",
    "#Seasonal 18.67%     é–‹å­¸ã€æƒ…äººç¯€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "import os.path\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://localhost:27017/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "db = client['fanpage_research']\n",
    "coll_fanpage = db['fanpage']\n",
    "coll_post = db['post']\n",
    "coll_post_detail = db['post_detail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('fid_list_4.json') as d:\n",
    "    all_fid_list = json.load(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1283,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "jieba.set_dictionary(\"dict.txt.big\")\n",
    "stop_words_path = \"data/household_stop_words.txt\"\n",
    "jieba.analyse.set_stop_words(stop_words_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pp(o):\n",
    "    print json.dumps(o,indent=1,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_combined_message_by_label(X_array, y_array, label):\n",
    "    nd_X = np.array(X_array)\n",
    "    nd_y = np.array(y_array)\n",
    "    words = \"\"\n",
    "    for p in nd_X[nd_y==label]:\n",
    "        if p['message'] is not None:\n",
    "            words = words + p['message']\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_keyword(X_array, y_array, label):\n",
    "    combined_message = get_combined_message_by_label(X_array, y_array, label)\n",
    "    return jieba.analyse.extract_tags(combined_message, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### -----------------above functions are global functions----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1487,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class industry_data:\n",
    "    def __init__(self,industry_name_list,all_fid_list):\n",
    "        self.name_list = industry_name_list\n",
    "        self.name_abbr = None\n",
    "        self.all_fid_list = all_fid_list\n",
    "        self.fid_list = None\n",
    "        self.posts = None\n",
    "        self.train_post = None\n",
    "        self.train_label = None\n",
    "        self.test_post = None\n",
    "        self.test_label = None\n",
    "    def get_name(self):\n",
    "        name = \"\"\n",
    "        if self.name_abbr == None:\n",
    "            for i in self.name_list:\n",
    "                name = name + i.rsplit(' ')[0]\n",
    "            self.name_abbr = name\n",
    "        return self.name_abbr\n",
    "    def get_fid_list(self,refresh=False):\n",
    "        if self.fid_list == None or refresh == True:\n",
    "            fid_list = []\n",
    "            for f in self.all_fid_list:\n",
    "                if f['category'] in self.name_list:\n",
    "                    fid_list.append(f['fid'])\n",
    "            self.fid_list = fid_list\n",
    "        return self.fid_list\n",
    "    def get_posts(self,refresh=False):\n",
    "        fid_list = self.get_fid_list()\n",
    "        if self.posts == None or refresh == True:\n",
    "            posts = []\n",
    "            for f in fid_list:\n",
    "                posts = posts + [p for p in coll_post.find({\"fid\":f},{'date':False,'_id':False})]\n",
    "            self.posts = posts\n",
    "        return self.posts\n",
    "    def get_train_post_label(self,sample_number=50,refresh=False):\n",
    "        self.train_label == None\n",
    "        if self.train_post == None or self.train_label == None or refresh == True:\n",
    "            file_name = 'data/train_data_'+self.get_name()+'.json'\n",
    "            if refresh == True or (not os.path.isfile(file_name)):\n",
    "                tmp_dict = {} \n",
    "                posts = self.get_posts()\n",
    "                tmp_dict['train_X'] = [p for p in np.random.choice(posts, 50, replace=False)]\n",
    "                tmp_dict['train_y'] = []\n",
    "                with open(file_name,'w') as outfile:\n",
    "                    json.dump(tmp_dict,outfile)\n",
    "            with open(file_name,'r') as infile:\n",
    "                j_file = json.load(infile)\n",
    "            try:\n",
    "                self.train_post = j_file['train_X']\n",
    "                self.train_label = j_file['train_y']\n",
    "            except:\n",
    "                print \"error occured when accessing\"+file_name+\"[train_X] or [train_y]\"          \n",
    "        return self.train_post,self.train_label\n",
    "    def get_test_post_label(self, sample_number = 50, refresh=False):\n",
    "        self.test_label == None\n",
    "        if self.test_post == None or self.test_label == None or refresh == True:\n",
    "            file_name = 'data/test_data_'+self.get_name()+'.json'\n",
    "            if refresh == True or (not os.path.isfile(file_name)):\n",
    "                tmp_dict = {} \n",
    "                train_X, train_y = self.get_train_post_label()\n",
    "                all_posts = self.get_posts()\n",
    "                posts_without_trainX = [i for i in all_posts if i not in train_X]\n",
    "                tmp_dict['test_X'] = [p for p in np.random.choice(posts_without_trainX, 50, replace=False)]\n",
    "                tmp_dict['test_y'] = []\n",
    "                with open(file_name,'w') as outfile:\n",
    "                    json.dump(tmp_dict,outfile)\n",
    "            with open(file_name,'r') as infile:\n",
    "                j_file = json.load(infile)\n",
    "            try:\n",
    "                self.test_post = j_file['test_X']\n",
    "                self.test_label = j_file['test_y']\n",
    "            except:\n",
    "                print \"error occured when accessing\"+file_name+\"[test_X] or [test_y]\"          \n",
    "        return self.test_post,self.test_label    \n",
    "    def get_keyword_category(self):\n",
    "        X, y = self.get_train_post_label()\n",
    "        keyword_dict = collections.OrderedDict()\n",
    "        if len(X)==len(y) and len(X)!=0:\n",
    "            for i in range(4):\n",
    "                combined_message = get_combined_message_by_label(X, y, i)\n",
    "                keyword_dict['class_'+str(i)] = jieba.analyse.extract_tags(combined_message, 10)\n",
    "        return keyword_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1685,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self,industry):\n",
    "        self.name = industry\n",
    "        self.keyword_dict = None\n",
    "    def get_keyword_dict(self,refresh=False):\n",
    "        if self.keyword_dict == None or refresh == True:\n",
    "            file_name = 'data/keyword_'+self.name+'.json'\n",
    "            if os.path.isfile(file_name):\n",
    "                with open(file_name,'r') as infile:\n",
    "                    keyword_file = collections.OrderedDict(json.load(infile))\n",
    "                self.keyword_dict = keyword_file\n",
    "            else:\n",
    "                print file_name+\" doesn't exist\"\n",
    "                return \n",
    "        return self.keyword_dict\n",
    "    \n",
    "# Below is stupid model\n",
    "#     def _predict(self,message):\n",
    "#         message_jieba = jieba.analyse.extract_tags(message,0)\n",
    "#         key_dict = self.get_keyword_dict()\n",
    "        \n",
    "#         print 'message jieba : ',','.join(message_jieba)\n",
    "#         print 'keyword 0',','.join(key_dict['class_0'])\n",
    "#         print 'keyword 1 : ',','.join(key_dict['class_1'])\n",
    "#         print 'keyword 2',','.join(key_dict['class_2'])\n",
    "        \n",
    "#         match_class_0 = list(set(message_jieba).intersection(key_dict['class_0']))\n",
    "#         match_class_1 = list(set(message_jieba).intersection(key_dict['class_1']))\n",
    "#         match_class_2 = list(set(message_jieba).intersection(key_dict['class_2']))\n",
    "\n",
    "#         print \"0: \", ','.join(match_class_0)\n",
    "#         print \"1: \", ','.join(match_class_1)\n",
    "#         print \"2: \", ','.join(match_class_2)\n",
    "#         if len(match_class_1)>0:\n",
    "#             if len(match_class_1)==1 and u'åˆ†äº«' in match_class_1:\n",
    "#                 if u'ç²‰çµ²' in message_jieba:   #ç²‰çµ²ï¼‹åˆ†äº« --> 1\n",
    "#                     return 1\n",
    "#                 else:\n",
    "#                     if len(match_class_0)>0:  #åˆ†äº«ï¼‹é€å¥½ç¦®-->1\n",
    "#                         return 1                    \n",
    "#                     return 3\n",
    "#             return 1\n",
    "#         elif len(match_class_0)>0:\n",
    "#             return 0\n",
    "#         elif len(match_class_2)>0:\n",
    "#             return 2\n",
    "#         else:\n",
    "#             return 3\n",
    "#     def predict(self,data_set):\n",
    "#         pre_y = []\n",
    "#         predict_class = None\n",
    "#         for d in data_set:\n",
    "#             if d['message']!=None:\n",
    "#                 pre_y.append(self._predict(d['message']))\n",
    "#                 predict_class = self._predict(d['message'])\n",
    "#             else:\n",
    "#                 if d['post_type'] in ['album','photo','cover_photo','video','video_inline','image_share','animated_image_share','video_direct_response']:\n",
    "#                     pre_y.append(1)\n",
    "#                     predict_class = 1\n",
    "#                 else:\n",
    "#                     pre_y.append(3)\n",
    "#                     predict_class = 3\n",
    "#             print predict_class\n",
    "#             print d\n",
    "#             print '---------------------------'\n",
    "#         return pre_y\n",
    "#Above is stupid model\n",
    "\n",
    "    def _make_ML_X(self,input_X,print_out=False):\n",
    "        key_dict = self.get_keyword_dict()\n",
    "        ML_X = []\n",
    "        for obj in input_X:\n",
    "            tmp = []\n",
    "            has_vivid = 0\n",
    "            if obj['post_type'] in ['album','photo','cover_photo','video','video_inline','image_share','animated_image_share','video_direct_response']:\n",
    "                has_vivid = 1\n",
    "            match = []\n",
    "            obj_message = obj['message']\n",
    "            for k in key_dict:\n",
    "                if obj_message!=None:\n",
    "                    message_jieba = jieba.analyse.extract_tags(obj['message'],0)\n",
    "                    match = list(set(message_jieba).intersection(key_dict[k]))\n",
    "                if print_out:\n",
    "                    print 'match '+k+' : ',','.join(match)\n",
    "                tmp.append(len(match))\n",
    "            tmp.append(has_vivid)\n",
    "            ML_X.append(tmp)\n",
    "        return ML_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Machine Learning Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1700,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ML:\n",
    "    # industry_name is a list obj\n",
    "    def __init__(self,industry_name_list):\n",
    "        self.industry_obj = industry_data(industry_name_list,all_fid_list)\n",
    "        self.train_X, self.train_y = self.industry_obj.get_train_post_label()\n",
    "        self.test_X, self.test_y = self.industry_obj.get_test_post_label()\n",
    "        self.classifier = Classifier(self.industry_obj.name_abbr)\n",
    "        self.ML_train_X = self.classifier._make_ML_X(self.train_X)\n",
    "        self.ML_test_X = self.classifier._make_ML_X(self.test_X)\n",
    "        self.model_lg = None\n",
    "        self.model_tree = None\n",
    "    def get_model_tree(self,refresh=False):\n",
    "        if self.model_tree == None or refresh == True:\n",
    "            clf = tree.DecisionTreeClassifier()\n",
    "            clf_model = clf.fit(np.array(self.ML_train_X),np.array(self.train_y))\n",
    "            self.model_tree = clf_model\n",
    "        return self.model_tree\n",
    "    def get_model_lg(self,refresh=False):\n",
    "        if self.model_lg == None or refresh == True:\n",
    "            lg = LogisticRegression()\n",
    "            lg_model = lg.fit(np.array(self.ML_train_X),np.array(self.train_y))\n",
    "            self.model_lg = lg_model\n",
    "        return self.model_lg\n",
    "    def _predict(self,ML_X,model='lg'):\n",
    "        prediction = None\n",
    "        if model == \"lg\":\n",
    "            prediction = self.get_model_lg().predict(np.array(ML_X))\n",
    "        elif model == \"tree\":\n",
    "            prediction = self.get_model_tree().predict(np.array(ML_X))\n",
    "        return prediction\n",
    "    def accuracy(self,ML_X,y,model = 'lg'):\n",
    "        prediction = self._predict(ML_X,model)\n",
    "        accuracy = np.mean(prediction==np.array(y))\n",
    "        correct_idx = np.where(prediction==np.array(y))[0]\n",
    "        incorrect_idx = np.where(prediction!=np.array(y))[0]\n",
    "        print \"accuracy:\",accuracy\n",
    "        print \"correct_idx:\",correct_idx\n",
    "        print \"incorrect_idx:\",incorrect_idx \n",
    "    def checkOne(self,X,y,index,model='lg'):\n",
    "        ML_X = self.classifier._make_ML_X([X[index]],print_out = True)\n",
    "        print \"predict class : \", self._predict(np.reshape(np.array(ML_X),(1,-1)),model)\n",
    "        print \"true class : \", y[index]\n",
    "        print X[index]['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1728,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.84\n",
      "correct_idx: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 22 23 24 25 26 27\n",
      " 28 29 30 31 32 34 35 36 37 38 39 40 41 42 43 47 48]\n",
      "incorrect_idx: [ 2 20 21 33 44 45 46 49]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "retail_ML = ML(['retail'])\n",
    "print retail_ML.accuracy(retail_ML.ML_test_X, retail_ML.test_y,\"lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1729,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match class_2 :  ç”¢å“\n",
      "match class_3 :  \n",
      "match class_0 :  \n",
      "match class_1 :  \n",
      "predict class :  [2]\n",
      "true class :  3\n",
      "CITIZEN Watch Groupå±•ç¤ºå»³\n",
      "\n",
      "ä»Šå¹´BASELWORLDçš„CITIZENå±•å ´ç•¶ä¸­\n",
      "æœ‰å¦ä¸€å€‹äº®é»ï¼ï¼CITIZEN Watch Groupçš„å±•ç¤ºå»³ã€‚\n",
      "\n",
      "åœ¨é€™æˆ‘å€‘èšé›†\n",
      "CITIZEN Watch Groupç¾åœ¨æ‰€æ“æœ‰çš„æ‰‹éŒ¶å“ç‰Œ ï¼š\n",
      "Alpinaã€Arnold & Sonã€BULOVAã€\n",
      "CAMPANOLA ã€CITIZEN ä»¥åŠ Ferderiue Constant .\n",
      "\n",
      "åœ¨é€™å»£å¤§çš„ç©ºé–“å†…å±•ç¤ºå„å“ç‰Œçš„æœ€æ–°ç”¢å“ï¼Œ\n",
      "ä¸åƒ…ä¾¿æ–¼å±•ç¾å„è‡ªç¨ç‰¹çš„ç¾å­¸ç†å¿µï¼Œ\n",
      "ä¹Ÿèƒ½æ›´å¥½çš„èˆ‡å¤§çœ¾æºé€šCITIZEN Watch Groupçš„ç¾å¥½é¡˜æ™¯ã€‚\n",
      "\n",
      "ã€‰BASELWORLD 2017\n",
      " http://www.citizenwatch.com.tw/event2017/baselworld/index.html\n"
     ]
    }
   ],
   "source": [
    "retail_ML.checkOne(retail_ML.test_X,retail_ML.test_y,2,\"lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1734,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.96\n",
      "correct_idx: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48]\n",
      "incorrect_idx: [ 7 49]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "house_ML = ML(['Household & Personal Care'])\n",
    "print house_ML.accuracy(house_ML.ML_test_X, house_ML.test_y,\"lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1737,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match class_2 :  è©¦ç”¨\n",
      "match class_3 :  æ´»å‹•\n",
      "match class_0 :  å¾—ç,å¥½ç¦®,å„ªæƒ ,å¾—çè€…\n",
      "match class_1 :  åŠ å…¥\n",
      "predict class :  [0]\n",
      "true class :  1\n",
      "ã€å¥½å¥‡å¯¶å¯¶èšæ¨‚éƒ¨ ã€Œè¿æ–°å…¥æœƒå°¿å¸ƒå…è²»æŠ½ã€- ğŸ‰å¾—çåå–®ğŸ‰å‡ºçˆå›‰ï¼ï¼ã€‘\n",
      "\n",
      "æ­¡è¿æ–°åŠ å…¥çš„åª½å’ªå€‘â¤æ›´æ­å–œé€™å€‹æœˆæŠ½åˆ°âœ¨äº”ç®±ç´«å¥½å¥‡âœ¨çš„å¹¸é‹å…’ï¼Œè®“äººå¥½ç”Ÿç¾¨æ…•å•ŠğŸ˜ğŸ˜ğŸ˜~~ã€‚ä½ ä¹Ÿæƒ³æŠŠäº”ç®±ç´«å¥½å¥‡æ¬å›å®¶å—ï¼Ÿæ´»å‹•æ–¼4/30æˆªæ­¢ï¼Œé‚„æ²’åŠ å…¥æœƒå“¡çš„åª½å’ªå€‘å¿«ä¾†åŠ å…¥ã€Œå¥½å¥‡å¯¶å¯¶èšæ¨‚éƒ¨ã€å§ï¼\n",
      "å¿ƒå‹•ä¸å¦‚é¦¬ä¸Šè¡Œå‹•ğŸ ç«‹å³åŠ å…¥å¥½å¥‡å¯¶å¯¶èšæ¨‚éƒ¨: https://goo.gl/bhmiGO\n",
      "\n",
      "å°ˆå±¬å¥½å¥‡å¯¶å¯¶èšæ¨‚éƒ¨æœƒå“¡å››å¤§å¥½ç¦®: \n",
      "ğŸ‘å¾—åˆ°å„é …é€šè·¯èˆ‡ç”¢å“çš„å„ªæƒ è³‡è¨Š\n",
      "ğŸ‘æ–°ç”Ÿå¯¶å¯¶å°ˆå±¬ç¦®\n",
      "ğŸ‘ç”¢å“è©¦ç”¨åŒ…\n",
      "ğŸ‘å­•æœŸ/è‚²å…’æ–°çŸ¥é›»å­å ±\n",
      "\n",
      "ğŸ‰ å¾—çè€…å€‘è¨˜å¾—åœ¨3/24(äº”)å‰ä¸»å‹•è¯çµ¡æˆ‘å€‘å”·ï¼Œè«‹ä¾†é›»0800-221-526æˆ–mailè¯çµ¡è³‡è¨Šè‡³customerservice.0800@kcc.comï¼ \n",
      "è¯çµ¡è³‡è¨Šè«‹æä¾›Facebookå§“ååŠç™»å…¥ç”¨ä¹‹Emailã€çœŸå¯¦å§“åã€é›»è©±ã€æ”¶ä»¶åœ°å€ï¼Œå°‡ç”±å°ˆäººå”åŠ©é ˜çã€‚\n",
      "å¾—çè€…æ•ä¸å¦è¡Œé€šçŸ¥ï¼Œé€¾æœŸå›è¦†å°‡è¦–åŒæ£„æ¬Šï¼Œå¦‚æœ‰ä»»ä½•å•é¡Œè«‹æ–¼ä¸Šç­æ™‚é–“(é€±ä¸€è‡³é€±äº” 09:00-12:00 / 13:30-17:00)æ´½è©¢å¥½å¥‡å®¢æœå°ˆç·š 0800-221 -526ã€‚\n"
     ]
    }
   ],
   "source": [
    "house_ML.checkOne(house_ML.test_X,house_ML.test_y,7,\"lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Household + Retail & ä¸åˆ†keyword list (keyword_all.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1815,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9\n",
      "correct_idx: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 27 28 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51\n",
      " 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 72 73 74 75 76 77 79 80\n",
      " 81 82 83 84 85 86 87 88 89 90 91 92 93 97 98]\n",
      "incorrect_idx: [26 29 52 70 71 78 94 95 96 99]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "RetailHouse = ML(['Retail','Household & Personal Care'])\n",
    "print RetailHouse.accuracy(RetailHouse.ML_test_X, RetailHouse.test_y,\"lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1829,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match product_test :  \n",
      "match product_new :  \n",
      "match info_event :  \n",
      "match monetary_incentive :  \n",
      "match product_unit :  \n",
      "match price_off :  75æŠ˜\n",
      "match engage_other :  \n",
      "match product_version :  \n",
      "match info_brand :  \n",
      "match push_word :  \n",
      "match engage_join :  \n",
      "match engage_interaction :  \n",
      "predict class :  [2]\n",
      "true class :  2\n",
      "ğŸ’¢æ˜æ˜å¾ˆç´¯å»ç¡ä¸è‘—\n",
      "ä»Šæ™šåˆè¦å¤±çœ äº†å—ï¼ŸğŸ˜µ\n",
      "åˆ¥æ²®å–ªï¼è©¦è©¦é€™æ‹›â†“â†“\n",
      "ğŸ’¤ç¡å‰ç”¨æ™šå®‰èˆ’çœ å™´éœ§\n",
      "å°è‘—æ•é ­ã€æ£‰è¢«ã€è‡¥å®¤\n",
      "å’»ï¼å’»ï¼å™´å€‹å¹¾ä¸‹\n",
      "é¦¬ä¸Šæ•£ç™¼ç”œæ©™å’Œè–°è¡£è‰é¦™æ°›ğŸŠ\n",
      "æ•´å€‹äººè®Šå¾—å¥½æ”¾é¬†\n",
      "ä»Šæ™šä¸€å®šå¥½å¥½ç¡ï¼\n",
      "\n",
      "æ™šå®‰èˆ’çœ å™´éœ§2ç“¶çµ„â†˜75æŠ˜\n",
      "https://goo.gl/QanGwx\n",
      "\n",
      "è½‰è¼‰è‡ªè¯äººå¥åº·ç¶²\n",
      "\n",
      " â€” Products shown: æ™šå®‰èˆ’çœ å™´éœ§2ç“¶çµ„.\n"
     ]
    }
   ],
   "source": [
    "RetailHouse.checkOne(RetailHouse.test_X,RetailHouse.test_y,2,\"lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1830,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 19, 36, 29])"
      ]
     },
     "execution_count": 1830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(RetailHouse.train_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
