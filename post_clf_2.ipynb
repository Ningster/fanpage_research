{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將 training data set 標記類別，儲存為excel格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import xlwt\n",
    "import json\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def json_to_excel(train_or_test,json_file_name, excel_file_name):\n",
    "    with open('data/'+json_file_name,'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    wb = xlwt.Workbook()\n",
    "    ws = wb.add_sheet(\"My Worksheet\")\n",
    "    for idx,post in enumerate(json_file[train_or_test+'_X']):\n",
    "        ws.write(idx, 0, label = post['pid'])\n",
    "        ws.write(idx, 1, label = post['message'])\n",
    "        ws.write(idx, 2, label = post['post_type'])\n",
    "    wb.save(excel_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#json_to_excel('train',\"train_data_all_2.json\",\"train_post_class.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 輸出新的分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_myclass_list(xls_path):\n",
    "    myclass_list = []\n",
    "    wb = xlrd.open_workbook(xls_path)\n",
    "    sh = wb.sheet_by_index(0)\n",
    "    for rownum in range(1, sh.nrows):\n",
    "        row_values = sh.row_values(rownum)\n",
    "        myclass_list.append(int(row_values[4]))\n",
    "    return myclass_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_new_class(train_or_test):\n",
    "    myclass_list = get_myclass_list(train_or_test+\"_post_class.xls\")\n",
    "    with open('data/'+train_or_test+'_data_all_2.json','r') as f:\n",
    "        load_dict = json.load(f)\n",
    "        load_dict[train_or_test+'_y'] = myclass_list\n",
    "    with open('data/'+train_or_test+'_data_all_2.json','w') as f:\n",
    "        json.dump(load_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#change_new_class('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結巴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jieba.set_dictionary(\"dict.txt.big\")\n",
    "jieba.analyse.set_stop_words(\"data/household_stop_words.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextMining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextMining:\n",
    "    def __init__(self,keyword_dict_path):\n",
    "        self.keyword_dict_path = keyword_dict_path\n",
    "        self.keyword_dict = None\n",
    "    def get_keyword_dict(self,refresh=False):\n",
    "        if self.keyword_dict == None or refresh == True:\n",
    "            with open(self.keyword_dict_path,'r') as infile:\n",
    "                self.keyword_dict = json.load(infile, object_pairs_hook=OrderedDict)\n",
    "        return self.keyword_dict\n",
    "    def make_feature_vector(self,post_dict,print_out=False):\n",
    "        key_dict = self.get_keyword_dict()\n",
    "        feature_vector = []\n",
    "        resultList = list(jieba.cut(post_dict['message'])) if post_dict['message'] is not None else []\n",
    "        for typology in key_dict:\n",
    "            word_count_list = [resultList.count(word) for word in key_dict[typology]]\n",
    "            total_count = sum(word_count_list) if len(resultList)!=0 else 0\n",
    "            if typology==\"service\":\n",
    "                total_count += 1 if post_dict['post_type'] == 'share' else 0\n",
    "            if typology==\"engage_photovideo_direction\":\n",
    "                total_count += 1 if post_dict['post_type'] != 'share' else 0\n",
    "            feature_vector.append(total_count)\n",
    "            if print_out :\n",
    "                match_word = \"\"\n",
    "                for idx, word_count in enumerate(word_count_list):\n",
    "                    if word_count != 0:\n",
    "                        match_word+=key_dict[typology][idx]+' count '+str(word_count)+','\n",
    "                print \"%s (term count :%d)  %s\" % (typology,total_count,match_word)\n",
    "        return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trans_feature_vector(X,print_out=False):\n",
    "    feature_vector_list = []\n",
    "    all_TextMining = TextMining(\"data/keyword_all_2.json\")\n",
    "    for post in X :\n",
    "        feature_vector_list.append(all_TextMining.make_feature_vector(post,print_out))\n",
    "    return feature_vector_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ML:\n",
    "    def __init__(self,method):\n",
    "        self.method = method\n",
    "        self.model = None\n",
    "    def get_model(self,train_X,train_y,refresh=False):\n",
    "        # 將post dict 轉換成數字向量\n",
    "        train_X_feature_vector_list = trans_feature_vector(train_X)\n",
    "        if self.model == None or refresh == True:\n",
    "            if self.method == 'lg':\n",
    "                clf = LogisticRegression()\n",
    "            elif self.method == 'svm':\n",
    "                clf = svm.SVC()\n",
    "            else:\n",
    "                print \"no method name \", self.method\n",
    "                return\n",
    "            clf_model = clf.fit(np.array(train_X_feature_vector_list),np.array(train_y))\n",
    "            self.model = clf_model\n",
    "        return self.model\n",
    "    def _predict(self,X,print_out=False):\n",
    "        # 將post dict 轉換成數字向量\n",
    "        X_feature_vector_list = trans_feature_vector(X,print_out)\n",
    "        if self.model != None:\n",
    "            prediction = self.model.predict(np.array(X_feature_vector_list))\n",
    "            return prediction\n",
    "        else:\n",
    "            return None\n",
    "    def accuracy(self,X,y):\n",
    "        prediction = self._predict(X)\n",
    "        if prediction is not None:\n",
    "            accuracy = np.mean(prediction==np.array(y))\n",
    "            correct_idx = np.where(prediction==np.array(y))[0]\n",
    "            incorrect_idx = np.where(prediction!=np.array(y))[0]\n",
    "            print \"accuracy:\",accuracy\n",
    "            print \"correct_idx:\",correct_idx\n",
    "            print \"incorrect_idx:\",incorrect_idx \n",
    "        else:\n",
    "            print \"prediction is None\"\n",
    "    def checkOne(self,X,y,idx):\n",
    "        prediction = self._predict([X[idx]],print_out=True)\n",
    "        print \"-------------------------------------------------\"\n",
    "        print \"predict class : \", prediction\n",
    "        print \"true class : \", y[idx]\n",
    "        print X[idx]['message']\n",
    "    def _classification_report(self,X,y):\n",
    "        prediction = self._predict(X)\n",
    "        target_names = ['class 0 促銷', 'class 1 客戶', 'class 2 產品','class 3 品牌']\n",
    "        report = classification_report(y, prediction, target_names=target_names)\n",
    "        print report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 載入訓練資料\n",
    "with open(\"data/train_data_all_2.json\",\"r\") as r_f:\n",
    "    train_data = json.load(r_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ML_lg = ML('lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_lg.get_model(train_data['train_X'],train_data['train_y'],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.656666666667\n",
      "correct_idx: [  0   2   4   5   6   7   8  10  11  13  15  16  19  22  23  24  27  28\n",
      "  29  31  32  33  34  35  36  38  39  40  41  43  44  45  47  48  49  50\n",
      "  51  53  54  55  56  57  58  59  60  61  62  63  64  65  66  68  69  75\n",
      "  78  80  82  85  86  89  93  94  95  96  99 102 104 106 109 110 111 112\n",
      " 113 115 118 119 120 121 123 124 125 126 128 130 132 133 134 135 136 137\n",
      " 138 140 142 143 144 145 148 149 150 152 153 154 156 157 158 159 161 162\n",
      " 163 164 166 167 168 169 170 171 172 173 174 175 177 178 182 183 184 187\n",
      " 188 189 190 192 193 194 195 196 198 202 203 205 206 209 210 212 214 215\n",
      " 216 217 218 219 220 222 224 226 227 228 229 230 236 237 238 239 240 242\n",
      " 244 245 246 248 252 254 255 256 257 259 260 261 262 263 265 266 267 271\n",
      " 272 273 276 277 278 280 283 284 285 286 287 288 289 290 295 297 298]\n",
      "incorrect_idx: [  1   3   9  12  14  17  18  20  21  25  26  30  37  42  46  52  67  70\n",
      "  71  72  73  74  76  77  79  81  83  84  87  88  90  91  92  97  98 100\n",
      " 101 103 105 107 108 114 116 117 122 127 129 131 139 141 146 147 151 155\n",
      " 160 165 176 179 180 181 185 186 191 197 199 200 201 204 207 208 211 213\n",
      " 221 223 225 231 232 233 234 235 241 243 247 249 250 251 253 258 264 268\n",
      " 269 270 274 275 279 281 282 291 292 293 294 296 299]\n"
     ]
    }
   ],
   "source": [
    "ML_lg.accuracy(train_data['train_X'],train_data['train_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand (term count :0)  \n",
      "social (term count :0)  \n",
      "service (term count :1)  \n",
      "engage_assist (term count :1)  關鍵 count 1,\n",
      "engage_like (term count :0)  \n",
      "engage_question_appreciate (term count :0)  \n",
      "engage_photovideo_direction (term count :0)  \n",
      "product_new (term count :0)  \n",
      "product_version (term count :0)  \n",
      "product_unit (term count :0)  \n",
      "product_test (term count :0)  \n",
      "promotion_deal (term count :0)  \n",
      "promotion_chance (term count :0)  \n",
      "season (term count :0)  \n",
      "-------------------------------------------------\n",
      "predict class :  [3]\n",
      "true class :  1\n",
      "做油飯很困難很麻煩？一點也不！\n",
      "其實只要掌握這五個關鍵，油飯幾乎不會失敗還保證簡單又好吃！\n",
      "蠔油真的很萬用，配上各式各樣的食材真的都能烹調出最美味的佳餚！\n",
      "\n",
      "#李錦記甘甜醬油露\n",
      "#李錦記舊庄特級蠔油\n"
     ]
    }
   ],
   "source": [
    "ML_lg.checkOne(train_data['train_X'],train_data['train_y'],14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "class 0 促銷       0.71      0.51      0.59        47\n",
      "class 1 客戶       0.74      0.52      0.61        82\n",
      "class 2 產品       0.61      0.84      0.71       118\n",
      "class 3 品牌       0.66      0.58      0.62        53\n",
      "\n",
      "   avg / total       0.67      0.66      0.65       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ML_lg._classification_report(train_data['train_X'],train_data['train_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
